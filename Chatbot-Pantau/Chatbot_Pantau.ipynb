{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import & Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import hashlib\n",
    "import logging\n",
    "import traceback\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document, SystemMessage, HumanMessage, AIMessage\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize API Key for OpenAI\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Initialize LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY, temperature=0.5)\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(filename='data_loading.log', level=logging.WARNING)\n",
    "\n",
    "# Cache for processed files\n",
    "processed_files = set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_hash(filepath):\n",
    "    \"\"\"Create a hash for a file to check if it has been processed.\"\"\"\n",
    "    hasher = hashlib.md5()\n",
    "    with open(filepath, 'rb') as f:\n",
    "        hasher.update(f.read())\n",
    "    return hasher.hexdigest()\n",
    "\n",
    "def is_file_processed(filepath):\n",
    "    \"\"\"Check if a file has already been processed.\"\"\"\n",
    "    hash_value = file_hash(filepath)\n",
    "    if hash_value in processed_files:\n",
    "        return True\n",
    "    processed_files.add(hash_value)\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf_to_vectorstore(pdf_path):\n",
    "    \"\"\"Load PDF and split into smaller documents.\"\"\"\n",
    "    try:\n",
    "        loader = PyPDFLoader(pdf_path)\n",
    "        documents = loader.load()\n",
    "        splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "        documents = splitter.split_documents(documents)\n",
    "        \n",
    "        # Add metadata (source)\n",
    "        for doc in documents:\n",
    "            doc.metadata = {\"source\": pdf_path}\n",
    "        \n",
    "        return documents\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Error loading PDF: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return []\n",
    "\n",
    "def validate_csv_data(df):\n",
    "    \"\"\"Validate and clean CSV data.\"\"\"\n",
    "    df = df.dropna(subset=['Title', 'Content'])\n",
    "    df = df[df['Content'].str.len() > 50]\n",
    "    return df\n",
    "\n",
    "def remove_duplicates(df):\n",
    "    \"\"\"Remove duplicates from the DataFrame.\"\"\"\n",
    "    return df.drop_duplicates(subset=['Title', 'Content'])\n",
    "\n",
    "def load_csv_to_vectorstore(csv_path):\n",
    "    \"\"\"Load data from CSV and prepare for vector store.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        df = validate_csv_data(df)\n",
    "        df = remove_duplicates(df)\n",
    "\n",
    "        documents = [\n",
    "            Document(page_content=f\"{row['Title']}\\n\\n{row['Content']}\", metadata={\"source\": csv_path})\n",
    "            for _, row in df.iterrows()\n",
    "        ]\n",
    "\n",
    "        splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "        documents = splitter.split_documents(documents)\n",
    "        \n",
    "        return documents\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Error loading CSV: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and Load Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_store(documents):\n",
    "    \"\"\"Create a vector store from documents using embeddings.\"\"\"\n",
    "    embedding_function = OpenAIEmbeddings(api_key=OPENAI_API_KEY, model=\"text-embedding-3-small\")\n",
    "    vectorstore = Chroma(\n",
    "        collection_name=\"my_collection\",\n",
    "        embedding_function=embedding_function,\n",
    "        persist_directory=\"chroma_vectorstore\"\n",
    "    )\n",
    "    \n",
    "    vectorstore.add_documents(documents)\n",
    "    vectorstore.persist()\n",
    "    return vectorstore\n",
    "\n",
    "def load_or_create_vectorstore(pdf_path, csv_path):\n",
    "    \"\"\"Load or create the vector store.\"\"\"\n",
    "    embedding_function = OpenAIEmbeddings(api_key=OPENAI_API_KEY, model=\"text-embedding-3-small\")\n",
    "    \n",
    "    # Check if a vector store already exists and load it if available\n",
    "    if os.path.exists(\"chroma_vectorstore\"):\n",
    "        print(\"Memuat vectorstore yang sudah ada...\")\n",
    "        vectorstore = Chroma(\n",
    "            collection_name=\"my_collection\",\n",
    "            embedding_function=embedding_function,\n",
    "            persist_directory=\"chroma_vectorstore\"\n",
    "        )\n",
    "        return vectorstore\n",
    "    else:\n",
    "        print(\"Membuat vectorstore baru...\")\n",
    "        pdf_docs = load_pdf_to_vectorstore(pdf_path)\n",
    "        csv_docs = load_csv_to_vectorstore(csv_path)\n",
    "        combined_docs = pdf_docs + csv_docs\n",
    "        \n",
    "        if not combined_docs:\n",
    "            raise ValueError(\"No documents were loaded successfully.\")\n",
    "        \n",
    "        return create_vector_store(combined_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatbot Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store conversation history\n",
    "messages = [SystemMessage(content=\"Anda adalah asisten politik yang memberikan jawaban dalam bahasa Indonesia.\")]\n",
    "\n",
    "def get_gpt4_answer(human_message):\n",
    "    \"\"\"Get an answer from the GPT-4 model.\"\"\"\n",
    "    messages.append(HumanMessage(content=human_message))\n",
    "    response = llm.invoke(messages)\n",
    "    messages.append(AIMessage(content=response.content))\n",
    "    return response.content\n",
    "\n",
    "def search_and_answer(human_message, vectorstore):\n",
    "    \"\"\"Search for relevant documents and generate an answer using the LLM.\"\"\"\n",
    "    try:\n",
    "        search_results = vectorstore.similarity_search(human_message, k=3)\n",
    "        if not search_results:\n",
    "            return \"Maaf, saya tidak memiliki informasi tentang pertanyaan tersebut.\"\n",
    "        \n",
    "        # Menggunakan set untuk menyimpan sumber unik\n",
    "        unique_sources = set()\n",
    "        context = \"\"\n",
    "        \n",
    "        for doc in search_results:\n",
    "            source = doc.metadata.get('source', 'Tidak diketahui')\n",
    "            if source not in unique_sources:\n",
    "                unique_sources.add(source)\n",
    "                context += f\"Sumber: {source} - {doc.page_content}\\n\"\n",
    "\n",
    "        if not context:\n",
    "            return \"Maaf, saya tidak menemukan informasi yang relevan.\"\n",
    "\n",
    "        messages.append(SystemMessage(content=f\"Context: {context}\"))\n",
    "        \n",
    "        # Generate the answer\n",
    "        answer = get_gpt4_answer(human_message)\n",
    "        return f\"Pertanyaan: {human_message}\\n\\nJawaban: {answer}\\n\\nInformasi ini berasal dari:\\n{context}\"\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Error in search_and_answer: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return \"Terjadi kesalahan dalam pencarian.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memuat vectorstore yang sudah ada...\n",
      "Masuk ke dalam loop...\n",
      "Chatbot: Pertanyaan: siapa itu fufufafa?\n",
      "\n",
      "Jawaban: Fufufafa adalah nama akun yang sedang menjadi sorotan, terutama terkait dengan pernyataan Menteri Komunikasi dan Informatika (Menkominfo) yang menyatakan niat untuk mengungkap identitas pemilik akun tersebut. Namun, informasi lebih lanjut mengenai siapa sebenarnya pemilik akun ini atau konteks spesifiknya mungkin perlu dicari dari sumber berita terbaru atau laporan resmi. Jika ada perkembangan lebih lanjut, biasanya media akan memberikan informasi yang lebih lengkap.\n",
      "\n",
      "Informasi ini berasal dari:\n",
      "Sumber: D:\\Chatbot-Pantau\\Database\\fufufafa_clean_no_url.csv - Pemilik Akun Fufufafa Mau Diungkap Menkominfo, Siapa Dia?\n",
      "\n",
      "Chatbot: Terima kasih, sampai jumpa!\n"
     ]
    }
   ],
   "source": [
    "# Main loop program\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = r\"D:\\Chatbot-Pantau\\Database\\4208-12183-3-PB.pdf\"\n",
    "    csv_path = r\"D:\\Chatbot-Pantau\\Database\\fufufafa_clean_no_url.csv\"\n",
    "\n",
    "    try:\n",
    "        # Load or create vectorstore\n",
    "        vectorstore = load_or_create_vectorstore(pdf_path, csv_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Gagal membuat atau memuat vectorstore: {e}\")\n",
    "        exit(1)\n",
    "\n",
    "    print(\"Masuk ke dalam loop...\")\n",
    "    while True:\n",
    "        try:\n",
    "            pertanyaan = input(\"Kamu: \")\n",
    "            if pertanyaan.lower() == \"exit\":\n",
    "                print(\"Chatbot: Terima kasih, sampai jumpa!\")\n",
    "                break\n",
    "            elif pertanyaan.lower() == \"lihat riwayat\":\n",
    "                for msg in messages:\n",
    "                    if isinstance(msg, HumanMessage):\n",
    "                        print(f\"Kamu: {msg.content}\")\n",
    "                    elif isinstance(msg, AIMessage):\n",
    "                        print(f\"Chatbot: {msg.content}\")\n",
    "                continue\n",
    "\n",
    "            jawaban = search_and_answer(pertanyaan, vectorstore)\n",
    "            print(f\"Chatbot: {jawaban}\")\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nProgram dihentikan oleh pengguna.\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Terjadi kesalahan: {e}\")\n",
    "            traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
